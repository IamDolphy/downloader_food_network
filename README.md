This is a snippet from a project in archiving data from foodnetwork. The goal was to
crawl the page links and download each page without triggering any potential security systems or tying up their servers. 
The total download was over 5gb. The best solution was simply creating a delay between each download as there were no evidence 
of anti bot measures in place. Assumptions were made that navigation on their newer pages are the same on the older pages even though 
the layouts on older pages did not match the new ones. 
